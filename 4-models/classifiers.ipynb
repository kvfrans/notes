{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import flax\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import functools\n",
    "from einops import rearrange\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "\n",
    "> A **classifier** is a model that is trained to classify data into one of N distinct categories.\n",
    "\n",
    "One of the simplest possible models to train is the **categorical classifier**. In this setting, we have a labelled dataset consisting of `(data, label) / (x,y)` pairs, where each **label** (y) is an integer representing one of N distinct classes. We would then like to train our classifier model such that it correctly predicts which class a given datapoint (x) falls into.\n",
    "\n",
    "## Probability Maximization\n",
    "\n",
    "To dervive the classification objective, we'll first note that **a classifier is a probability distribution over classes**. We want to fit the parameters of our classifier model to maximize the probability of samples from our dataset. To do this, we minimize the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "L(\\theta) = - \\sum_{x, y \\sim D} \\log p_\\theta(y|x).\n",
    "$$\n",
    "\n",
    "This loss function is also referred to as the **cross-entropy loss** in machine learning literature. The name refers to the fact that this loss is in fact equivalent to information-theoretic cross-entropy between the data distribution and the model distribution.\n",
    "\n",
    "\n",
    "## Model Structure\n",
    "\n",
    "The final output of a neural network classifier must represent a probability distribution over classes. In the discrete categorical setting, this is simple -- the model can output a vector of probabilities, one for each possible class.\n",
    "\n",
    "However, probability distributions must be positive and sum to one, and it is tricky to enforce these contraints in a neural network setting. Therefore, models are instead trained to output **logits**, which are unconstrained and real-valued. To get the probabilities from the logits, we exponentiate then explicitly normalize the values, dividing by their total sum. This operation is so common that is gets a special name, the **softmax** operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits): # [num_classes]\n",
    "    return jnp.exp(logits) / jnp.sum(jnp.exp(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting these together, we can now implement a basic classification loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return nn.Dense(10)(x)\n",
    "\n",
    "def loss(params, inputs, labels):\n",
    "    logits = Classifier().apply({'params': params}, inputs)\n",
    "    one_hot_labels = jax.nn.one_hot(labels, 10)\n",
    "    probs = softmax(logits)\n",
    "    log_probs = jnp.log(probs)\n",
    "    return -jnp.mean(jnp.sum(one_hot_labels * log_probs, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Imagenet Classification\n",
    "\n",
    "To get a feel for a practical-size classifier, let's train a model that learns to classify the Imagenet dataset, a set of 1.2 million images with 1000 classes. We will use our homebrew `jaxtransformer` library to provide the backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtransformer import TransformerBackbone\n",
    "from jaxtransformer.modalities import Patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we maximize log-probabilities instead of probabilities?\n",
    "\n",
    "Because numerically, log-probabilities are easier to work with. First, note that the `log` function is monotonically increasing, so maximizing log-probabilities is equivalent to maximizing probabilites. Working in log space brings a number of numerical benefits. The log-probability of an independent joint distribution is the *sum* of individual log-probs, whereas raw probabilities would need to be multplied. Multiplication, especially of small values, will quickly become numerically unstable. We would much rather work with\n",
    "\n",
    "$$\n",
    "\\sum_{y,x} \\; \\log p_\\theta(y|x) \\qquad \\text{instead of} \\qquad \\prod_{y,x} \\; p_\\theta(y|x),\n",
    "$$\n",
    "\n",
    "and the optimal parameters are the same in either case.\n",
    "\n",
    "## Why so some libraries subtract a constant in their softmax implementations?\n",
    "\n",
    "The raw softmax function can be numerically unstable, as it involves an `exp` operator. To alleviate this problem, practical machine learning libraries tend to make use of a trick in the softmax equation: since the softmax is a ratio, we can scale all terms by a constant and get the same result. In logit-space, this means we can subtract a constant **before the exp** and get the same result. Often, the maximum logit is subtracted from the logit vector, which reduces the change that `exp` is called on a large number.\n",
    "\n",
    "## What is the difference between binary cross-entropy and cross-entropy?\n",
    "\n",
    "The term **binary cross-entropy** often arises when classifying data into two possible categories. The equation for binary cross-entropy is in fact the same as cross-entropy, just written in a slightly different way.\n",
    "\n",
    "$$\n",
    "\\underbrace{\\log p_\\theta(y|x)}_{\\text{cross-entropy}} = \\sum_{y'} \\log p_\\theta(y'|x) * ùüô(y' = y)  = \\underbrace{\\log p_\\theta(y_1) * ùüô(y_1 = y) + \\log p_\\theta(y_0) * ùüô(y_0 = y)}_{\\text{binary cross-entropy}}\n",
    "$$\n",
    "\n",
    "Additionally, the binary equivalent of the softmax activation is the **sigmoid** function, which gives the probability $p(y_1)$ from a real-valued logit, using the fact that for binary classification, $p(y_0) = 1 - p(y_1)$. Using $h_n$ to refer to logits for class labels $y_n$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y) = \\frac{exp(h)}{\\sum_{h'} exp(h')} \\qquad \\rightarrow \\qquad p(y_1) & = \\frac{exp(h_1)}{exp(h_1) + exp(h_0)} \\quad \\text{(binary case)}. \\\\\n",
    "& = \\frac{1}{1 + exp(h_1 - h_0)} \\\\\n",
    "& = \\frac{1}{1 + e^{h}} \\quad \\text{(sigmoid function)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note for the binary case, probabilty is a function purely of the difference between logits $h_1$ and $h_0$, so we only need to learn a single logit.\n",
    "\n",
    "## Can classification labels be not one-hot?\n",
    "\n",
    "Yes, it's perfectly valid to have a dataset where labels are not one-hot, but rather full probability vectors. This is when the interpretation of minimizing cross-entropy between two distributions comes in helpful:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum_{x \\sim D} \\sum_{y'} \\log p_\\theta(y'|x) * p^*(y')\n",
    "$$\n",
    "where $p^(y')$ represents the probability vector given by the dataset, or a teacher mdoel, etc. Often, the inner sum over possible labels is computed in vectorized form as a dot product. Full probability targets are most often used in model distillation, to copy the behavior of a teacher model into a student model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "\n",
    "- Problem Setting\n",
    "- Cross-Entropy Loss\n",
    "- Overfitting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
