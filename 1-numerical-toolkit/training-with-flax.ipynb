{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import flax\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building NNs with Flax\n",
    "\n",
    "On top of JAX, we will use **Flax** which is a higher-level framework for building neural networks. Flax is still decently simplistic (we don't get a one-line `keras.fit`) and should be seen as a collection of helpful primitives we can put together how we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a gradient in JAX\n",
    "\n",
    "Built-in to JAX is an **autograd** engine, which lets us automatically compute gradients of functions with `jax.grad`. We can take a pure function mapping `x` to `y`, and get a new function which calculates the gradient of `y` with respect to `x`. \n",
    "\n",
    "Our high-level strategy will be to write our neural networks using Flax, then use `jax.grad` to run gradient descent and minimize loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(x=8) = 2*(x-2) = 12.0\n"
     ]
    }
   ],
   "source": [
    "def loss(x):\n",
    "    return jnp.square(x - 2).mean()\n",
    "grad_x = jax.grad(loss) # grad(x) = 2 * (x-2)\n",
    "print('grad(x=8) = 2*(x-2) =', grad_x(8.0)) # 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flax Modules\n",
    "\n",
    "The main abstraction of Flax is a **module**, which represents some kind of *computation* along with relevant *parameters*. Modules are composable, so we can construct complicated modules out of simpler ones. We will view everything from a feedforward layer to an entire neural network as a module.\n",
    "\n",
    "Flax separtes the concept of the computation graph and the actual parameters. When we write a Flax module, we can declare parameters with an **initialization function** that describes how to create them (e.g. sample from a normal distribution). These parameters are treated as placeholders until we actually call `init` on the module, at which we can handle the parameters as a separate object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: [[-0.606425   -0.58962685 -0.4824153 ]\n",
      " [ 0.33814558  1.2511139   0.02361481]\n",
      " [ 0.5528764  -0.15400036  0.06896964]]\n",
      "Output: [-3.2329245  2.9112177  0.4517846]\n"
     ]
    }
   ],
   "source": [
    "import flax.linen as nn\n",
    "class MyModule(nn.Module):\n",
    "    output = 3\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        weights = self.param('weights', nn.initializers.lecun_normal(), \n",
    "                            (self.output, x.shape[-1]))\n",
    "        return weights @ x\n",
    "\n",
    "x = jnp.array([1,2,3])\n",
    "m = MyModule()\n",
    "k = jax.random.key(42)\n",
    "params = m.init(k, x) # Initialize the module, which creates parameters.\n",
    "y = m.apply(params, x) # Call the module\n",
    "print('Parameters:', params['params']['weights'])\n",
    "print('Output:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we defined our module from scratch, but in general we can compose our networks out of built-in Flax primitives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.LayerNorm(x)\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = jnp.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytrees\n",
    "Flax modules naturally have a tree-like structure. For example, the `MyNet` module above has an `nn.Dense` sub-module, which contains `kernel` and `bias` parameters. We want a nice way to reason about trees without having to know their structure.\n",
    "\n",
    "JAX uses the **pytree** as a helpful abstraction. Pytrees are just native Python objects -- they can be dictionaries, lists, or tuples. JAX provides `jax.tree_utils` helper functions, which can operate over pytrees of arbitrary structure. For example, if we wanted to add ten to every parameter in `params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'params': {'weights': Array([[-0.606425  , -0.58962685, -0.4824153 ],\n",
      "       [ 0.33814558,  1.2511139 ,  0.02361481],\n",
      "       [ 0.5528764 , -0.15400036,  0.06896964]], dtype=float32)}}\n",
      "params_new: {'params': {'weights': Array([[ 9.393575,  9.410373,  9.517585],\n",
      "       [10.338145, 11.251114, 10.023615],\n",
      "       [10.552876,  9.846   , 10.06897 ]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "# This works regardless if `params` is a list, a dictionary, a list of dicts, etc.\n",
    "params = m.init(k, x)\n",
    "params_new = jax.tree_util.tree_map(lambda x : x + 10, params)\n",
    "print('params:', params)\n",
    "print('params_new:', params_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Let's build a simple **gradient descent** algorithm out of these building blocks. We'll start by defining a simple neural network, then we will write down our loss function, and pass the entire thing through `jax.grad`. We will iteratively compute the gradient and adjust our parameters slightly, minimizing our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: loss =  0.50568265\n",
      "Iter 3: loss =  0.07405198\n",
      "Iter 6: loss =  0.061948203\n",
      "Iter 9: loss =  0.06161511\n"
     ]
    }
   ],
   "source": [
    "class MyNet(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = nn.relu(x)\n",
    "        return nn.Dense(1)(x)\n",
    "\n",
    "x = jnp.arange(128) / 128\n",
    "y_true = jnp.sin(x)\n",
    "\n",
    "net = MyNet()\n",
    "k = jax.random.key(42)\n",
    "params = net.init(k, x)\n",
    "\n",
    "def loss_fn(params, x):\n",
    "    y_pred = net.apply(params, x)\n",
    "    return jnp.mean((y_pred - y_true)**2)\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(loss_fn))\n",
    "\n",
    "for i in range(10):\n",
    "    loss, grad = loss_and_grad(params, x)\n",
    "    params = jax.tree_util.tree_map(lambda p, g : p - g*0.005, params, grad)\n",
    "    if i % 3 == 0:\n",
    "        print(f'Iter {i}: loss = ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing our gradient descent step by hand is nice for understanding, but Flax also supports the `optax` package which gives us a set of common optimizers to use. To achieve the same behavior as above, we can use `optax.sgd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: loss =  0.50568265\n",
      "Iter 3: loss =  0.07405198\n",
      "Iter 6: loss =  0.061948203\n",
      "Iter 9: loss =  0.06161511\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "tx = optax.sgd(learning_rate=0.005)\n",
    "params = net.init(k, x)\n",
    "opt_state = tx.init(params) # This is empty for SGD, but some optimizers need it.\n",
    "for i in range(10):\n",
    "    loss, grad = loss_and_grad(params, x)\n",
    "    updates, opt_state = tx.update(grad, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 3 == 0:\n",
    "        print(f'Iter {i}: loss = ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use Adam instead, we can just change `optax.sgd` to `optax.adam`. Optax has clean implementations of AdamW too for weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train State\n",
    "\n",
    "A common paradigm when using Flax is to define a **training state** that contains all the relevant things for our model. We will want to store the Flax module, its parameters, the optimization state, and the learning step. It can also be convenient to keep the RNG state here too. If the run ever crashes, as long as we can recover the training state we have everything we need to keep going. \n",
    "\n",
    "Generally, we will JIT an `update` function that returns a new `TrainState` object each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(flax.struct.PyTreeNode):\n",
    "    params: Any\n",
    "    opt_state: Any\n",
    "    step: int\n",
    "\n",
    "@jax.jit\n",
    "def update(train_state, x):\n",
    "    loss, grad = loss_and_grad(params, x)\n",
    "    updates, opt_state = tx.update(grad, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return train_state.replace(params=params, opt_state=opt_state, \n",
    "                               step=train_state.step+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
