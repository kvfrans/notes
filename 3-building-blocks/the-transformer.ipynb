{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import flax\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import functools\n",
    "from einops import rearrange\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer\n",
    "\n",
    "<!-- - History\n",
    "- Transformer Structure\n",
    "- Input Heads\n",
    "- Output Heads -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ubiquitous neural network architecture today is the **transformer**. Originating from origins in language modelling, the Transformer has proven to have empirically powerful scaling properties, and makes almost no domain-specific assumptions. Today, transformers are used across the board, even in image or robotic control domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architectural Diagram\n",
    "\n",
    "The transformer is a residual network, with residual blocks comprising of dense layers and self-attention layers. The transformer is a **set-operator**, meaning the activations at stage are a *set* of feature vectors. Positional information within this set is represented via positional encodings applied to each vector. We will cover the various input/output heads of the transformer later.\n",
    "\n",
    "**Residual blocks**. The specific details of residual blocks vary between each model. We will describe the **GPT-2** architecture here. In GPT-2, each residual block consists of:\n",
    "- Layernorm on the residual stream vectors.\n",
    "- Multi-headed self attention.\n",
    "- A residual connection, plus a second Layernorm.\n",
    "- Two dense layers, with a GeLU activation between.\n",
    "\n",
    "Each attention/dense layer is applied in *parallel* among the entire set of feature vectors. This is why transformers can make very efficient use of GPU time -- even if the batch size is small, the true batch size of each operation is z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
