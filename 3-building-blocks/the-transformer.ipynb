{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import flax\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import functools\n",
    "from einops import rearrange\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer\n",
    "\n",
    "> The **Transformer** is a popular architecture of choice today.\n",
    "> - The core of the transformer is simpler than it may seem, and **we implement it in 20 lines of code**.\n",
    "> - Transformers are domain-agnostic. They can be applied to text, images, video, etc.\n",
    "\n",
    "## History\n",
    "The Transformer architecture is closely intertwined with the [attention operator](attention.ipynb). Researchers working on natural language translation found that augmenting a traditional recurrent network with attention layers could increase accuracy. Later, it was found that attention was so effective, the recurrent connections could be dropped entirely -- hence the title \"[Attention is all you need](https://arxiv.org/abs/1706.03762)\" in the original Transformer paper. Today, transformers are used not only in langauge, but across the board in image, video, robotics, and so on.\n",
    "\n",
    "<!-- - History\n",
    "- Transformer Structure\n",
    "- Input Heads\n",
    "- Output Heads -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architectural Diagram\n",
    "\n",
    "The core of a transformer is a [residual network](residual-networks.ipynb), where each layer is a *set* of feature tokens. The residual blocks comprise of a **self-attention** layer, in which information can be shared within the set of tokens, as well as **dense layers** that operate independently on each token in the set. \n",
    " \n",
    "The specific details of residual blocks vary between kinds of transformer models. We will describe the **GPT-2** architecture here. In GPT-2, each residual block consists of:\n",
    "- LayerNorm on the residual stream vectors.\n",
    "- Multi-headed self-attention.\n",
    "- A residual connection, plus a second LayeNorm.\n",
    "- Two dense layers, with a GeLU activation between.\n",
    "\n",
    "Each attention/dense layer is applied in *parallel* among the entire set of feature tokens. This is why transformers can make very efficient use of GPU time -- even if the true batch size is small, the effective batch size of each dense layer is `batch_size * num_tokens`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer](transformer.png)\n",
    "> **Diagram of a transformer block**. Note that every operator **except attention (colored background)** is computed independently for each token. The attention operator is the only time in which tokens can communicate information to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many transformer implementations online come with bloated features, settings, etc. I want to stress that **the core of a Transformer is incredibly simple**. We can implement in only a twenty lines of code, and we will do it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 10, 128)\n",
      "Output shape: (1, 10, 128)\n"
     ]
    }
   ],
   "source": [
    "class TransformerBackbone(nn.Module):\n",
    "    num_features: int = 128\n",
    "    num_blocks: int = 8\n",
    "    num_heads: int = 4\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x): # x: [batch, tokens, features]\n",
    "        channels_per_head = self.num_features // self.num_heads\n",
    "        for _ in range(self.num_blocks):\n",
    "            # Attention block.\n",
    "            y = nn.LayerNorm()(x)\n",
    "            k, q, v = [nn.Dense(self.num_features)(y) for _ in range(3)]\n",
    "            k, q, v = [jnp.reshape(p, (x.shape[0], x.shape[1], \n",
    "                        self.num_heads, channels_per_head)) for p in [k, q, v]]\n",
    "            q = q / jnp.sqrt(q.shape[3])\n",
    "            w = jnp.einsum('bqhc,bkhc->bhqk', q, k).astype(jnp.float32)\n",
    "            w = nn.softmax(w, axis=-1)\n",
    "            y = jnp.einsum('bhqk,bkhc->bqhc', w, q)\n",
    "            y = jnp.reshape(y, x.shape)\n",
    "            y = nn.Dense(self.num_features)(y)\n",
    "            x = x + y\n",
    "            # MLP block.\n",
    "            y = nn.LayerNorm()(x)\n",
    "            y = nn.Dense(self.num_features * 4)(y)\n",
    "            y = nn.gelu(y)\n",
    "            y = nn.Dense(self.num_features)(y)\n",
    "            x = x + y\n",
    "        return x\n",
    "    \n",
    "net = TransformerBackbone()\n",
    "input = jnp.zeros((1, 10, 128))\n",
    "params = net.init(jax.random.PRNGKey(0), input)['params']\n",
    "output = net.apply({'params': params}, input)\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look inside popular transformer implementations, at some point you will find a module that looks like the one we just defined. This transformer trunk is where the bulk of the processing and computation takes places. It is a blessing that regardless of the data type, we can use a transformer -- know the transformer well, and you will be prepared for most settings.\n",
    "\n",
    "The remaining layers of a transformer are the small **input heads** and **output heads** that surround the trunk. These heads will change based on the data format, and we will describe two common ones below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Heads\n",
    "\n",
    "Today, many have adopted the transformer as a default network architecture, regardless of domain. What remains domain-specific are the specific input and output heads required to transform the raw data into a token representation. Remember that in a transformer, the input to the network is a set of tokens -- each which is a real-valued vector.\n",
    "\n",
    "In language, the raw data is a set of words, represented as discrete integers. To turn these integers into tokens, we use an **embedding layer**, which is just a lookup table. Under the hood, the embedding layer is a `[vocab_size, num_features]` matrix. To encode a given word, we simply take the corresponding feature vector in the embedding layer matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 10)\n",
      "Output shape: (1, 10, 128)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingInput(nn.Module):\n",
    "    vocab_size: int = 256\n",
    "    num_features: int = 128\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x): # x is [batch, tokens (int)]\n",
    "        embedding_matrix = self.param('embedding', nn.initializers.xavier_uniform(), (self.vocab_size, self.num_features))\n",
    "        return jnp.take(embedding_matrix, x, axis=0)\n",
    "    \n",
    "net = EmbeddingInput()\n",
    "input = jnp.ones((1, 10), dtype=jnp.int32)\n",
    "params = net.init(jax.random.PRNGKey(0), input)['params']\n",
    "output = net.apply({'params': params}, input)\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For images, the raw data is a 2D matrix. To transform the image into a set of tokens, we use a **patch layer**, which breaks up the image into non-overlapping patches, using each patch as the initialization of a token. The patch layer is often implemented as a convolutional layer, followed by a reshape to turn the 2D patches into a token set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 128, 128, 3)\n",
      "Output shape: (1, 64, 128)\n"
     ]
    }
   ],
   "source": [
    "class PatchInput(nn.Module):\n",
    "    num_features: int = 128\n",
    "    patch_size: int = 16\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x): # x is [batch, height, width, colors]\n",
    "        patch_tuple = (self.patch_size, self.patch_size)\n",
    "        num_patches = (x.shape[1] // self.patch_size)\n",
    "        x = nn.Conv(self.num_features, patch_tuple, patch_tuple, padding=\"VALID\")(x) # (B, P, P, hidden_size)\n",
    "        x = rearrange(x, 'b h w c -> b (h w) c', h=num_patches, w=num_patches)\n",
    "        return x\n",
    "    \n",
    "net = PatchInput()\n",
    "input = jnp.zeros((1, 128, 128, 3))\n",
    "params = net.init(jax.random.PRNGKey(0), input)['params']\n",
    "output = net.apply({'params': params}, input)\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape) # 8*8 patches = 64 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Heads\n",
    "\n",
    "Likewise, we also need to define output heads to project our final tokens back to the raw data format. For text or classification tasks, we generally use a **classification head** that maps each token into a logit vector. For image outputs, we use an **patch output head**, that linearly projects each token back into the original patch size. Nothing fancy here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierOutput(nn.Module):\n",
    "    num_classes: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return nn.Dense(self.num_classes, global_dtype)(x)\n",
    "\n",
    "class PatchOutput(nn.Module):\n",
    "    patch_size: int\n",
    "    channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, c):\n",
    "        batch_size, num_patches, _ = x.shape\n",
    "        patch_side = int(num_patches ** 0.5)\n",
    "        x = nn.Dense(self.patch_size * self.patch_size * self.channels, dtype=global_dtype)(x)\n",
    "        x = jnp.reshape(x, (batch_size, patch_side, patch_side, self.patch_size, self.patch_size, self.channels))\n",
    "        x = jnp.einsum('bhwpqc->bhpwqc', x)\n",
    "        x = rearrange(x, 'B H P W Q C -> B (H P) (W Q) C', H=patch_side, W=patch_side)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Unified Architecture\n",
    "\n",
    "Today, most large-scale models use a transformer backbone. The transformer does not depend on domain-specific assumptions, which has allowed its widespread use. While there may be a shiny new architecture in the next years, the trend of a unifying architecture is likely to hold. \n",
    "\n",
    "![d](unified.png)\n",
    "> **Common large-scale models today.** The transformer backbone is almost identical in all settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
