{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import flax\n",
    "from typing import Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order Optimization\n",
    "\n",
    "Let's discuss optimization in the context of **second-order gradient descent**. The idea here is to use second-order gradient information in combination with first-order information to make more accurate update steps. The basic algorithm is also known as **Newton's method**. Examining the difference between first and second-order gradient updates:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta & \\leftarrow \\theta - \\alpha \\nabla_\\theta \\; L(\\theta)  & & \\text{(First-order gradient descent)}\\\\\n",
    "\\theta & \\leftarrow \\theta - \\alpha H(\\theta)^{-1} \\nabla_\\theta \\; L(\\theta)  & & \\text{(Second-order gradient descent)}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "is the presence of the $H(\\theta)^{-1}$ term, i.e. the **Hessian**, a matrix of second-order derivatives. By scaling our gradients with the inverse Hessian, we get a number of nice properties (which we will examine shortly). The downside of course is the cost; calculating $H(\\theta)$ itself is expensive, and inverting it even more so.\n",
    "\n",
    "In the rest of this page, we'll look at:\n",
    "- Interpretations of second-order descent\n",
    "- Approximations to computing $H(\\theta)^{-1}$ in classical optimization\n",
    "- Approximations to computing $H(\\theta)^{-1}$ in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-order descent as preconditioning\n",
    "\n",
    "A black-box way to view second-order descent is as using a specific type of **preconditioning**. Recall that a preconditioner is a linear transformation of the gradient. The intution is that certain gradient components are more sensitive, and we should update them more carefully than others. Using an appropriate preconditioner can greatly stabilize training in ill-conditioned cases, e.g. certain inputs have a much higher scale, etc. The inverse Hessian ends up being the 'correct' way to precondition a gradient update using second-order information, as shown next.\n",
    "\n",
    "## Second-order descent as solving a quadratic approximation\n",
    "\n",
    "We can approximate the true loss function using a second-order Taylor series expansion:\n",
    "\n",
    "$$\n",
    "\\tilde{L}(\\theta + \\theta') = L(\\theta) + \\nabla L(\\theta)^{T}\\theta' + \\dfrac{1}{2} \\theta'^{T} \\nabla^2 L(\\theta) \\theta'.\n",
    "$$\n",
    "\n",
    "Assuming $\\nabla^2 L(\\theta)$ is invertible, we can now solve for the $\\theta'$ that minimizes this approximate loss:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\nabla \\tilde{L}(\\theta + \\theta') = 0 \\\\\n",
    "& \\nabla L(\\theta) + \\nabla^2 L(\\theta) \\theta' = 0 \\\\\n",
    "& \\theta' = (\\nabla^2 L(\\theta))^{-1} \\nabla L(\\theta) \\\\\n",
    "& \\theta' = H(\\theta)^{-1} \\nabla L(\\theta) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which gives us our original second-order descent method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kevin todo notes**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
