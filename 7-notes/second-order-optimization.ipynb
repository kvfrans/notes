{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import flax\n",
    "from typing import Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order Optimization\n",
    "\n",
    "Let's discuss optimization in the context of **second-order gradient descent**. The idea here is to use second-order gradient information in combination with first-order information to make more accurate update steps. The basic algorithm is also known as **Newton's method**. Examining the difference between first and second-order gradient updates:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta & \\leftarrow \\theta - \\alpha \\nabla_\\theta \\; L(\\theta)  & & \\text{(First-order gradient descent)}\\\\\n",
    "\\theta & \\leftarrow \\theta - \\alpha H(\\theta)^{-1} \\nabla_\\theta \\; L(\\theta)  & & \\text{(Second-order gradient descent)}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "is the presence of the $H(\\theta)^{-1}$ term. This is a matrix so important we give it a name, the **Hessian**, and it is the matrix of all pairwise second-order derivatives. By scaling our gradients with the inverse Hessian, we get a number of nice properties (which we will examine shortly). The downside of course is the cost; calculating $H(\\theta)$ itself is expensive, and inverting it even more so.\n",
    "\n",
    "In the rest of this page, we'll look at:\n",
    "- Interpretations of second-order descent\n",
    "- Approximations to computing $H(\\theta)^{-1}$ in classical optimization\n",
    "- Approximations to computing $H(\\theta)^{-1}$ in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-order descent as preconditioning\n",
    "\n",
    "A black-box way to view second-order descent is as a specific type of **preconditioning**. Recall that a preconditioner is a linear transformation of the gradient, often written as a matrix:\n",
    "\n",
    "$$\n",
    "\\theta & \\leftarrow \\theta - P \\nabla_\\theta \\; L(\\theta).\n",
    "$$\n",
    "\n",
    "We often want to use preconditioning when we want to update certain parameters at different speeds. For example, a diagonal preconditioner can be used to specify per-parameter learning rates. In classical optimization problems, this can be helpful is we know that e.g. certain inputs have a much higher magnitude, etc. The inverse Hessian ends up being the 'correct' way to precondition a gradient update using second-order information, as shown next.\n",
    "\n",
    "## Second-order descent as solving a quadratic approximation\n",
    "\n",
    "We can approximate the true loss function using a second-order Taylor series expansion:\n",
    "\n",
    "$$\n",
    "\\tilde{L}(\\theta + \\theta') = L(\\theta) + \\nabla L(\\theta)^{T}\\theta' + \\dfrac{1}{2} \\theta'^{T} \\nabla^2 L(\\theta) \\theta'.\n",
    "$$\n",
    "\n",
    "Assuming $\\nabla^2 L(\\theta)$ is invertible, we can now solve for the $\\theta'$ that minimizes this approximate loss:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\nabla \\tilde{L}(\\theta + \\theta') = 0 \\\\\n",
    "& \\nabla L(\\theta) + \\nabla^2 L(\\theta) \\theta' = 0 \\\\\n",
    "& \\theta' = (\\nabla^2 L(\\theta))^{-1} \\nabla L(\\theta) \\\\\n",
    "& \\theta' = H(\\theta)^{-1} \\nabla L(\\theta) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which gives us our original second-order descent method.\n",
    "\n",
    "## Second-order descent as knowing how far to step\n",
    "\n",
    "A key property of a proper inverse Hessian is that it is positive definite -- in other words, matrix multiplying will never flip the sign of a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kevin todo notes**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
