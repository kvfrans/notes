{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import flax\n",
    "from typing import Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "> **Backpropgation** is a technique for calculating the gradients of neural networks.\n",
    "\n",
    "The essential recipe for training neural networks is gradient descent. We want to iteratively update our parameters $\\theta$ by their negative **gradient**:\n",
    "\n",
    "$$\n",
    "\\theta' \\leftarrow \\theta - \\alpha * \\underbrace{\\nabla_\\theta \\; \\text{loss}(f_\\theta(x), y).}_{gradient}\n",
    "$$\n",
    "\n",
    "The gradient is the same shape as our parameters, and tells us how to update them to reduce our loss function. So, how do we find out what this gradient is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "Remember that neural networks are just sequences of functions that feed into one other. We can use the **chain rule** to tell us how to calculate gradients of composed functions. Given a composition $y = f(g(x))$:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\; f(g(x)) = \\nabla_{g(x)} \\; f(g(x)) * \\nabla_x \\; g(x),\n",
    "$$\n",
    "i.e., we can take the gradients of the input/output of each sub-function and multiply them together, giving us the gradient of the entire composition.\n",
    "\n",
    "The chain rule works even if $f$ and $g$ are vector-valued functions. Using the chain rule, we know that we can recursively compute the gradients of complex neural networks by breaking them down into simple components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Gradient of a Two-Layer MLP\n",
    "\n",
    "Let's go through a simple two-layer neural network as an example. We'll derive the gradient of our network by hand. Let's assume we have a some inputs `x` and `y` and we would like to minimize mean squared error. Our network would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = jnp.array(np.random.randn(128, 16) / np.sqrt(16))\n",
    "b1 = jnp.array(np.random.randn(128) * 0.01)\n",
    "W2 = jnp.array(np.random.randn(1, 128) / np.sqrt(128))\n",
    "x = jnp.array(np.random.randn(16, 10))\n",
    "y = jnp.array(np.random.randn(1, 10))\n",
    "\n",
    "# Loss calculation\n",
    "h1 = W1 @ x\n",
    "h2 = h1 + b1[:, None]\n",
    "h3 = jnp.clip(h2, 0, None) # ReLU\n",
    "h4 = W2 @ h3\n",
    "loss = jnp.mean((h4 - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the gradient of the loss with relation to parameters `W1, b1, W2`. We can do this iteratively, starting from the back and working forwards. We will start with the gradient of the network output with respect to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss = 2 * (h4 - y) # d_(x^2) = 2x\n",
    "d_h4 = d_loss / y.size # Gradient of jnp.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the rest of the `h` gradients, we can work backwards, scaling the previous `h` gradient by the local gradient at each step. Remember that dense layers are matrix multiplications, so the gradient is simple: $\\nabla_W \\; (Wx) = x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_h3 = W2.T @ d_h4 # h4 = W2 @ h3\n",
    "d_h2 = d_h3 * (h2 > 0) # h3 = jnp.clip(h2, 0, None)\n",
    "d_h1 = d_h2 # h2 = h1 + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can calculate the gradients for the actual parameter vectors in terms of the `h` gradients. Each of these is a simple one-step operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_W2 = d_h4 @ h3.T # h4 = W2 @ h3\n",
    "d_b1 = jnp.mean(d_h2, axis=1) # h2 = h1 + b1[:, None]\n",
    "d_W1 = d_h1 @ x.T # h1 = W1 @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sanity check by comparing to `jax.grad`'s automatic differentation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grads match? True\n"
     ]
    }
   ],
   "source": [
    "def manual_grad(params, data_input, data_output):\n",
    "    W1, b1, W2 = params\n",
    "    h1 = W1 @ data_input\n",
    "    h2 = h1 + b1[:, None]\n",
    "    h3 = h2 * (h2 > 0)\n",
    "    h4 = W2 @ h3\n",
    "    loss = jnp.mean(jnp.square(h4 - data_output))\n",
    "\n",
    "    d_h4 = 2 * (h4 - data_output) / data_output.shape[1]\n",
    "    d_h3 = W2.T @ d_h4\n",
    "    d_h2 = d_h3 * (h2 > 0)\n",
    "    d_h1 = d_h2\n",
    "\n",
    "    d_W2 = d_h4 @ h3.T\n",
    "    d_b1 = jnp.mean(d_h2, axis=1)\n",
    "    d_W1 = d_h1 @ data_input.T\n",
    "\n",
    "    return [d_W1, d_b1, d_W2], loss\n",
    "manual_grad_output = manual_grad([W1, b1, W2], x, y)\n",
    "\n",
    "# JAX automatic gradient.\n",
    "def loss(params, data_input, data_output):\n",
    "    W1, b1, W2 = params\n",
    "    h1 = W1 @ data_input\n",
    "    h2 = h1 + b1[:, None]\n",
    "    h3 = h2 * (h2 > 0)\n",
    "    h4 = W2 @ h3\n",
    "    return jnp.mean(jnp.square(h4 - data_output))\n",
    "jax_grad = jax.grad(loss)\n",
    "jax_grad_output = jax_grad([W1, b1, W2], x, y)\n",
    "\n",
    "print('Grads match?', jnp.allclose(jax_grad_output[0], manual_grad_output[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward\n",
    "\n",
    "By now, you should see a structure in the way our gradient is computed. We first run through the neural network as normal, during the **forward pass**. We need to keep all the intermediate features in memory, since we will need them later. Next, we'll start from the loss function and move in reverse for the **backward pass**, which computes the gradients for each parameter. This is the **backpropagation** algorithm.\n",
    "\n",
    "## Automatic Differentiation\n",
    "\n",
    "Machine learning libraries today generally implement a version of automatic differentiation that lets us skip our gradient functions by hand. JAX gives us `jax.grad`, which we used in the example to sanity check our hand-written gradient.\n",
    "\n",
    "Under the hood, automatic differentiation libraries do exactly what we did above -- calculate gradients by taking a forward pass, storing the intermediate values, then doing a backwards pass. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we need to store intermediate values?\n",
    "\n",
    "Storing intermediate values lets us avoid unneccssary computation. It's possible to calculate the gradient of each parameter from scratch, but that would be computationally wasteful. If we don't have enough memory on the GPU to store every intermediate state, it's possible to re-compute certain portions during the backward pass. This trades off memory usage for computational complexity. \n",
    "\n",
    "## Does backpropagation work for any network?\n",
    "\n",
    "Backpropagation is a recursive algorithm, and relies on modules which are either composed of backprop-able sub-modules, or have a manually written forward and backward function. Most mathematical operations (e.g. matrix multiplication, element-wise operations) have an analytical derivative we can use. If we want to use a forward function that doesn't have a deriative, such as a rounding operation, we can't naively differentiate it with backpropagation. In some cases, we define a surrogate backward pass that approximates the gradient we want to use for training, as done in the [straight-through estimator](https://arxiv.org/abs/1308.3432)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
