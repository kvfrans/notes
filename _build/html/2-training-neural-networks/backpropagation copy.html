
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Backpropagation &#8212; An Alchemist&#39;s Guide to Deep Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=9a378b06" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '2-training-neural-networks/backpropagation copy';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">An Alchemist's Guide to Deep Learning</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Toolkit</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1-numerical-toolkit/overview-of-jax.html">Overview of JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-numerical-toolkit/training-with-flax.html">Building NNs with Flax</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Training Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">Why Neural Networks?</a></li>
<li class="toctree-l1"><a class="reference internal" href="backpropagation.html">Backpropagation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F2-training-neural-networks/backpropagation copy.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/2-training-neural-networks/backpropagation copy.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Backpropagation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule">Chain Rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-gradient-of-a-two-layer-mlp">Example: Gradient of a Two-Layer MLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-and-backward">Forward and Backward</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-differentiation">Automatic Differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#do-we-need-to-store-intermediate-values">Do we need to store intermediate values?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#does-backpropagation-work-for-any-network">Does backpropagation work for any network?</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="backpropagation">
<h1>Backpropagation<a class="headerlink" href="#backpropagation" title="Link to this heading">#</a></h1>
<p>The essential recipe for training neural networks is gradient descent. We want to iteratively update our parameters <span class="math notranslate nohighlight">\(\theta\)</span> by their negative <strong>gradient</strong>:</p>
<div class="math notranslate nohighlight">
\[
\theta' \leftarrow \theta - \alpha * \underbrace{\nabla_\theta \; \text{loss}(f_\theta(x), y).}_{gradient}
\]</div>
<p>The gradient is the same shape as our parameters, and tells us how to update them to reduce our loss function. So, how do we find out what this gradient is?</p>
<section id="chain-rule">
<h2>Chain Rule<a class="headerlink" href="#chain-rule" title="Link to this heading">#</a></h2>
<p>Remember that neural networks are just sequences of functions that feed into one other. We can use the <strong>chain rule</strong> to tell us how to calculate gradients of composed functions. Given a composition <span class="math notranslate nohighlight">\(y = f(g(x))\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\nabla_x \; f(g(x)) = \nabla_{g(x)} \; f(g(x)) * \nabla_x \; g(x),
\]</div>
<p>i.e., we can take the gradients of the input/output of each sub-function and multiply them together, giving us the gradient of the entire composition.</p>
<p>The chain rule works even if <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are vector-valued functions. Using the chain rule, we know that we can recursively compute the gradients of complex neural networks by breaking them down into simple components.</p>
</section>
<section id="example-gradient-of-a-two-layer-mlp">
<h2>Example: Gradient of a Two-Layer MLP<a class="headerlink" href="#example-gradient-of-a-two-layer-mlp" title="Link to this heading">#</a></h2>
<p>Let’s go through a simple two-layer neural network as an example. We’ll derive the gradient of our network by hand. Let’s assume we have a some inputs <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> and we would like to minimize mean squared error. Our network would look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">128</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Loss calculation</span>
<span class="n">h1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">@</span> <span class="n">x</span>
<span class="n">h2</span> <span class="o">=</span> <span class="n">h1</span> <span class="o">+</span> <span class="n">b1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">h3</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">h2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="c1"># ReLU</span>
<span class="n">h4</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">@</span> <span class="n">h3</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">h4</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We want to compute the gradient of the loss with relation to parameters <code class="docutils literal notranslate"><span class="pre">W1,</span> <span class="pre">b1,</span> <span class="pre">W2</span></code>. We can do this iteratively, starting from the back and working forwards. We will start with the gradient of the network output with respect to the loss function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_loss</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">h4</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># d_(x^2) = 2x</span>
<span class="n">d_h4</span> <span class="o">=</span> <span class="n">d_loss</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span> <span class="c1"># Gradient of jnp.mean()</span>
</pre></div>
</div>
</div>
</div>
<p>To compute the rest of the <code class="docutils literal notranslate"><span class="pre">h</span></code> gradients, we can work backwards, scaling the previous <code class="docutils literal notranslate"><span class="pre">h</span></code> gradient by the local gradient at each step. Remember that dense layers are matrix multiplications, so the gradient is simple: <span class="math notranslate nohighlight">\(\nabla_W \; (Wx) = x\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_h3</span> <span class="o">=</span> <span class="n">W2</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">d_h4</span> <span class="c1"># h4 = W2 @ h3</span>
<span class="n">d_h2</span> <span class="o">=</span> <span class="n">d_h3</span> <span class="o">*</span> <span class="p">(</span><span class="n">h2</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># h3 = jnp.clip(h2, 0, None)</span>
<span class="n">d_h1</span> <span class="o">=</span> <span class="n">d_h2</span> <span class="c1"># h2 = h1 + b1</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we can calculate the gradients for the actual parameter vectors in terms of the <code class="docutils literal notranslate"><span class="pre">h</span></code> gradients. Each of these is a simple one-step operation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_W2</span> <span class="o">=</span> <span class="n">d_h4</span> <span class="o">@</span> <span class="n">h3</span><span class="o">.</span><span class="n">T</span> <span class="c1"># h4 = W2 @ h3</span>
<span class="n">d_b1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_h2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># h2 = h1 + b1[:, None]</span>
<span class="n">d_W1</span> <span class="o">=</span> <span class="n">d_h1</span> <span class="o">@</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="c1"># h1 = W1 @ x</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s sanity check by comparing to <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code>’s automatic differentation engine.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">manual_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">data_input</span><span class="p">,</span> <span class="n">data_output</span><span class="p">):</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">params</span>
    <span class="n">h1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">@</span> <span class="n">data_input</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">h1</span> <span class="o">+</span> <span class="n">b1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">h3</span> <span class="o">=</span> <span class="n">h2</span> <span class="o">*</span> <span class="p">(</span><span class="n">h2</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">h4</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">@</span> <span class="n">h3</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">h4</span> <span class="o">-</span> <span class="n">data_output</span><span class="p">))</span>

    <span class="n">d_h4</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">h4</span> <span class="o">-</span> <span class="n">data_output</span><span class="p">)</span> <span class="o">/</span> <span class="n">data_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">d_h3</span> <span class="o">=</span> <span class="n">W2</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">d_h4</span>
    <span class="n">d_h2</span> <span class="o">=</span> <span class="n">d_h3</span> <span class="o">*</span> <span class="p">(</span><span class="n">h2</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">d_h1</span> <span class="o">=</span> <span class="n">d_h2</span>

    <span class="n">d_W2</span> <span class="o">=</span> <span class="n">d_h4</span> <span class="o">@</span> <span class="n">h3</span><span class="o">.</span><span class="n">T</span>
    <span class="n">d_b1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d_h2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d_W1</span> <span class="o">=</span> <span class="n">d_h1</span> <span class="o">@</span> <span class="n">data_input</span><span class="o">.</span><span class="n">T</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">d_W1</span><span class="p">,</span> <span class="n">d_b1</span><span class="p">,</span> <span class="n">d_W2</span><span class="p">],</span> <span class="n">loss</span>
<span class="n">manual_grad_output</span> <span class="o">=</span> <span class="n">manual_grad</span><span class="p">([</span><span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># JAX automatic gradient.</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">data_input</span><span class="p">,</span> <span class="n">data_output</span><span class="p">):</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">params</span>
    <span class="n">h1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">@</span> <span class="n">data_input</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">h1</span> <span class="o">+</span> <span class="n">b1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">h3</span> <span class="o">=</span> <span class="n">h2</span> <span class="o">*</span> <span class="p">(</span><span class="n">h2</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">h4</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">@</span> <span class="n">h3</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">h4</span> <span class="o">-</span> <span class="n">data_output</span><span class="p">))</span>
<span class="n">jax_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">jax_grad_output</span> <span class="o">=</span> <span class="n">jax_grad</span><span class="p">([</span><span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Grads match?&#39;</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">jax_grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">manual_grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Grads match? True
</pre></div>
</div>
</div>
</div>
</section>
<section id="forward-and-backward">
<h2>Forward and Backward<a class="headerlink" href="#forward-and-backward" title="Link to this heading">#</a></h2>
<p>By now, you should see a structure in the way our gradient is computed. We first run through the neural network as normal, during the <strong>forward pass</strong>. We need to keep all the intermediate features in memory, since we will need them later. Next, we’ll start from the loss function and move in reverse for the <strong>backward pass</strong>, which computes the gradients for each parameter. This is the <strong>backpropagation</strong> algorithm.</p>
</section>
<section id="automatic-differentiation">
<h2>Automatic Differentiation<a class="headerlink" href="#automatic-differentiation" title="Link to this heading">#</a></h2>
<p>Machine learning libraries today generally implement a version of automatic differentiation that lets us skip our gradient functions by hand. JAX gives us <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code>, which we used in the example to sanity check our hand-written gradient.</p>
<p>Under the hood, automatic differentiation libraries do exactly what we did above – calculate gradients by taking a forward pass, storing the intermediate values, then doing a backwards pass.</p>
</section>
<section id="do-we-need-to-store-intermediate-values">
<h2>Do we need to store intermediate values?<a class="headerlink" href="#do-we-need-to-store-intermediate-values" title="Link to this heading">#</a></h2>
<p>Storing intermediate values lets us avoid unneccssary computation. It’s possible to calculate the gradient of each parameter from scratch, but that would be computationally wasteful. If we don’t have enough memory on the GPU to store every intermediate state, it’s possible to re-compute certain portions during the backward pass. This trades off memory usage for computational complexity.</p>
</section>
<section id="does-backpropagation-work-for-any-network">
<h2>Does backpropagation work for any network?<a class="headerlink" href="#does-backpropagation-work-for-any-network" title="Link to this heading">#</a></h2>
<p>Backpropagation is a recursive algorithm, and relies on modules which are either composed of sub-modules, or have a manually written forward and backward function. Most mathematical operations have a simple backward function (e.g. matrix multiplication, element-wise operations). If we want to use a forward function that doesn’t have a deriative, such as a rounding operation, we can’t naively differentiate it with backpropagation. In some cases, we define a surrogate backward pass that approximates the gradient we want to use for training, as done in the <a class="reference external" href="https://arxiv.org/abs/1308.3432">straight-through estimator</a>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./2-training-neural-networks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule">Chain Rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-gradient-of-a-two-layer-mlp">Example: Gradient of a Two-Layer MLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-and-backward">Forward and Backward</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-differentiation">Automatic Differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#do-we-need-to-store-intermediate-values">Do we need to store intermediate values?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#does-backpropagation-work-for-any-network">Does backpropagation work for any network?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kevin Frans
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>