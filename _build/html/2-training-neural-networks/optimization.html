
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Optimization &#8212; An Alchemist&#39;s Guide to Deep Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=9a378b06" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '2-training-neural-networks/optimization';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Model Agnostic Meta Learning" href="../7-notes/model-agnostic-meta-learning.html" />
    <link rel="prev" title="Backpropagation" href="backpropagation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">An Alchemist's Guide to Deep Learning</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Toolkit</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1-numerical-toolkit/overview-of-jax.html">Overview of JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-numerical-toolkit/training-with-flax.html">Building NNs with Flax</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Training Neural Networks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="neural-networks.html">Why Neural Networks?</a></li>
<li class="toctree-l1"><a class="reference internal" href="backpropagation.html">Backpropagation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../7-notes/model-agnostic-meta-learning.html">Model Agnostic Meta Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F2-training-neural-networks/optimization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/2-training-neural-networks/optimization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-quadratic-loss-function">Example: Quadratic loss function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-in-terms-of-eigenvalues">Learning rate in terms of eigenvalues</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preconditioning">Preconditioning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#descent-on-neural-networks">Descent on Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-learning-rate-adagrad">Adaptive Learning Rate (Adagrad)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum">Momentum</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adam">Adam</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-optimization-results">Summary of optimization results</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimization">
<h1>Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h1>
<p>Within the umbrella of gradient descent, there are many varieties of strategies we can choose from. We usually refer these update choices as <strong>optimizers</strong>, which often take the form of modifying the gradient in some way to stabilize learning.</p>
<section id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h2>
<p>The simplest optimization strategy is just <strong>gradient descent</strong>. The idea is to iteratively query the gradient, then update the parameters to move slightly in that direction. Remember that we want to minimize loss, so we want to move in the <em>negative</em> direction of the gradient.</p>
<p>Gradients represent the <em>first-order</em> behavior of the function, so gradients are only accurate around a local neighborhood of the parameters. A nice analogy is to think about descending down a parabola by moving in its tangent line. The tangent is constantly changing, so we need to keep re-calculating our gradient as we update the parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradient Descent</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">grads</span> 
</pre></div>
</div>
<p>Note that the larger of a step we take, the more inaccurate our gradient direction is. To accomodate for this, we will use a small <strong>learning rate</strong> and scale down our gradient accordingly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradient Descent with learning rate.</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">grads</span> <span class="o">*</span> <span class="n">lr</span> 
</pre></div>
</div>
</section>
<section id="example-quadratic-loss-function">
<h2>Example: Quadratic loss function<a class="headerlink" href="#example-quadratic-loss-function" title="Link to this heading">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Isn’t the optimal answer just <span class="math notranslate nohighlight">\(x=0\)</span>?</strong> For our setting, yes. Usually quadratic loss is posed as <span class="math notranslate nohighlight">\(L(x) = x^TQx - b^Tx\)</span>, and the optimal point is at <span class="math notranslate nohighlight">\(Q^{-1}b\)</span>. But we can always use change-of-variables to make the gradient dynamics the same, e.g. <span class="math notranslate nohighlight">\(y = x - Q^{-1}b\)</span>. For ease of analysis, we’ll use the simpler equation with only <span class="math notranslate nohighlight">\(Q\)</span>.</p>
</aside>
<p>To understand some dynamics about gradient descent, let’s take a look at a simple example. We’ll look at a simple <strong>quadratic loss</strong> function, a classic example in optimization. We are given a loss function paramterized by <span class="math notranslate nohighlight">\(Q\)</span> such that <span class="math notranslate nohighlight">\(L(x) = x^TQx\)</span>, and we look to find <span class="math notranslate nohighlight">\(x\)</span> that minimizes the loss. We’ll use a <span class="math notranslate nohighlight">\((2,2)\)</span> matrix <span class="math notranslate nohighlight">\(Q\)</span>, so <span class="math notranslate nohighlight">\(x\)</span> is a 2-dimensional vector.</p>
<p>Let’s examine some simple runs of gradient descent. The key here is to demonstrate that <em>even for very simple functions</em>, choosing the correct learning rate is important. Remember that our learning rate is intuitively defines a local neighborhood over parameters where we assume our gradient is correct.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_optim</span><span class="p">(</span><span class="n">optim</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">16</span>
    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]])</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span> <span class="p">[</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]])</span>

    <span class="nd">@jax</span><span class="o">.</span><span class="n">vmap</span>
    <span class="k">def</span> <span class="nf">quadratic_loss</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">v</span>

    <span class="n">iter_points</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">9.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
        <span class="n">iter_points</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">grad_z</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">z</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">optim</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">)</span>

    <span class="c1"># Create a grid of points</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">XY</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">quadratic_loss</span><span class="p">(</span><span class="n">XY</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Create the plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>


    <span class="n">iter_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">iter_points</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iter_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iter_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;ro-&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">grad_z</span>
<span class="n">plot_optim</span><span class="p">(</span><span class="n">gradient_descent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bf0b638b7ff47d31321651acd68f2b8dccfdc42f2a0aed66e9e0a565fe6e574f.png" src="../_images/bf0b638b7ff47d31321651acd68f2b8dccfdc42f2a0aed66e9e0a565fe6e574f.png" />
</div>
</div>
<p>If our learning rate is too low, the network will take a long time to reach the minimum point. Slow convergence is especially true for quadratic loss functions (and mean-squared-error in deep learning), as the gradient magnitude approaches zero along with the loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent_slow</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">-</span> <span class="mf">0.02</span> <span class="o">*</span> <span class="n">grad_z</span>
<span class="n">plot_optim</span><span class="p">(</span><span class="n">gradient_descent_slow</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/71360410450ddc05bc18e9f54a97c395c91a63d9e41cf9ffaadd3ffd61129347.png" src="../_images/71360410450ddc05bc18e9f54a97c395c91a63d9e41cf9ffaadd3ffd61129347.png" />
</div>
</div>
<p>On the other hand, a high learning rate means we will overshoot our optimal parameters. If the update overshoots but still ends up at a lower loss, network will still eventually converge, but will oscillate between directions and have an overall slower convergence. If the update overshoots by too much, the parameters will completely diverge and approach infinity. Both of these are not great.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent_oscillate</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">-</span> <span class="mf">0.325</span> <span class="o">*</span> <span class="n">grad_z</span>
<span class="n">plot_optim</span><span class="p">(</span><span class="n">gradient_descent_oscillate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bce66bcc416b1846226677eec8c401d68fd30c885ddcce738b26935343119847.png" src="../_images/bce66bcc416b1846226677eec8c401d68fd30c885ddcce738b26935343119847.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent_diverge</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">-</span> <span class="mf">0.34</span> <span class="o">*</span> <span class="n">grad_z</span>
<span class="n">plot_optim</span><span class="p">(</span><span class="n">gradient_descent_diverge</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ade8de1ba4a1a210822162b142df18e065bf0ce9d71f18a4397357a204bddc18.png" src="../_images/ade8de1ba4a1a210822162b142df18e065bf0ce9d71f18a4397357a204bddc18.png" />
</div>
</div>
</section>
<section id="learning-rate-in-terms-of-eigenvalues">
<h2>Learning rate in terms of eigenvalues<a class="headerlink" href="#learning-rate-in-terms-of-eigenvalues" title="Link to this heading">#</a></h2>
<p>For our simple quadratic loss function, we can actually identify the exact optimal learning rate. Let’s start by examining our cost matrix Q. We’ve defined Q to be a <code class="docutils literal notranslate"><span class="pre">2x2</span></code> matrix, with eigenvalues <code class="docutils literal notranslate"><span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">6</span></code>.</p>
<p>We can rewrite <span class="math notranslate nohighlight">\(Q\)</span> as <span class="math notranslate nohighlight">\(R \Sigma R^T\)</span> equivalently, where <span class="math notranslate nohighlight">\(R\)</span> contains the two eigenvectors of <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">16</span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]])</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">R</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">@</span> <span class="n">R</span><span class="o">.</span><span class="n">T</span>

<span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Q:&#39;</span><span class="p">,</span> <span class="n">Q</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Eigenvalues:&#39;</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Eigenvectors (R):&#39;</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Q: [[ 1.19030117 -0.95670858]
 [-0.95670858  5.80969883]]
Eigenvalues: [1. 6.]
Eigenvectors (R): [[-0.98078528  0.19509032]
 [-0.19509032 -0.98078528]]
</pre></div>
</div>
</div>
</div>
<p>This decomposition will give us a clearer look at what gradient descent is doing at each step. We’ll use <span class="math notranslate nohighlight">\(y = R^Tx\)</span> to represent a rotation of <span class="math notranslate nohighlight">\(x\)</span> onto the eigenvector space:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
x' &amp; = x - \alpha \nabla x^T Q x \\
&amp; = x - \alpha Q x \\
&amp; = x - \alpha R \Lambda R^T x \\
&amp; = Ry - \alpha R \Lambda R^T Ry \\
&amp; = R(y - \alpha \Lambda y) \\
&amp; = R (I - \alpha \Lambda) y.
\end{align}
\end{split}\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Does this analysis hold for quadratic losses with a non-zero optimum?</strong> Yes, it still does. In that case, <span class="math notranslate nohighlight">\(y\)</span> should be the rotation of the <em>difference</em> between <span class="math notranslate nohighlight">\(x\)</span> and the optimal <span class="math notranslate nohighlight">\(x^*\)</span>. This distance decreases exponentially.</p>
</aside>
<p>Look at that! When we rotate <span class="math notranslate nohighlight">\(x\)</span> onto the eigenvector space of <span class="math notranslate nohighlight">\(Q\)</span>, we get a simple formula – each component of <span class="math notranslate nohighlight">\(y\)</span> decreases exponentially by the rate of <span class="math notranslate nohighlight">\((1 - \alpha \lambda)\)</span>. Each component <em>independently</em> decreases, regardless of the status of the other components in the vector. We can view the total loss as the sum of these components, squared and scaled by its equivalent eigenvalue.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">9.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eigenvectors</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">grad_x</span>

<span class="c1"># Plot the trajectory</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">loss_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">ys</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">loss_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">ys</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;First eigen component&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Second eigen component&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Total Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">grad_z</span>
<span class="n">plot_optim</span><span class="p">(</span><span class="n">gradient_descent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/06418135888b5c605893ea48dbe750650100f30a429ba5f8b3dc9d829a86411a.png" src="../_images/06418135888b5c605893ea48dbe750650100f30a429ba5f8b3dc9d829a86411a.png" />
<img alt="../_images/bf0b638b7ff47d31321651acd68f2b8dccfdc42f2a0aed66e9e0a565fe6e574f.png" src="../_images/bf0b638b7ff47d31321651acd68f2b8dccfdc42f2a0aed66e9e0a565fe6e574f.png" />
</div>
</div>
<p><strong>Fig Above</strong>: In our quadratic cost function, each eigenvector is an axis of the ellipse. The orange curve represents our point quickly approaching the wide axis, then the blue curve shows our point slowly converging along the second axis.</p>
<p>How can we find the <strong>optimal learning rate</strong> which will let us converge the fastest? We want to reach <span class="math notranslate nohighlight">\(y=0\)</span> (and therefore <span class="math notranslate nohighlight">\(x=0\)</span>) to minimize loss, so we are bounded by the slowest moving component of <span class="math notranslate nohighlight">\(y\)</span>. Remember that each component exponentially decreases by <span class="math notranslate nohighlight">\( |1 - \alpha \lambda|\)</span>.</p>
<p>The constraint is therefore as follows. Among all components, we want the slowest rate <span class="math notranslate nohighlight">\(|1 - \alpha \lambda|\)</span> to be the maximum value it can be (so we learn fast), <em>without</em> letting any <span class="math notranslate nohighlight">\( |1 - \alpha \lambda|\)</span> be greater than one (or we will diverge). For our <span class="math notranslate nohighlight">\(Q\)</span> matrix, this means our learning rate should be:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal LR is&#39;</span><span class="p">,</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal LR is 0.2857142857142857
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">-</span> <span class="mf">0.28</span> <span class="o">*</span> <span class="n">grad_z</span>
<span class="n">plot_optim</span><span class="p">(</span><span class="n">gradient_descent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/73ecb08de76eace4af44ee1803d1012b862b8e216873b323d8ca438762d7ccac.png" src="../_images/73ecb08de76eace4af44ee1803d1012b862b8e216873b323d8ca438762d7ccac.png" />
</div>
</div>
</section>
<section id="preconditioning">
<h2>Preconditioning<a class="headerlink" href="#preconditioning" title="Link to this heading">#</a></h2>
<p>In the above section, we had to make compromises on our learning rate because we asserted that learning rate was a global scalar. We can generalize the notion of a learning rate to more complicated functions. <strong>Preconditioned gradient descent</strong> assumes some matrix <span class="math notranslate nohighlight">\(P\)</span> which we’ll use to transform our gradient:</p>
<div class="math notranslate nohighlight">
\[
\theta \leftarrow \theta + \alpha P \nabla_\theta \; L(\theta)
\]</div>
<p>If <span class="math notranslate nohighlight">\(P\)</span> is a diagonal matrix, we can view preconditioning as having a separate learning rate for each coordinate of <span class="math notranslate nohighlight">\(x\)</span>. In our quadratic loss, the horizontal dimension is more stretched out, so we could try using a higher learning rate there:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">diagonal_gradient_descent</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">):</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">-</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">P</span> <span class="o">@</span> <span class="n">grad_z</span>
<span class="n">plot_optim</span><span class="p">(</span><span class="n">diagonal_gradient_descent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b218467ab7f2993b432f2247fe4d5c4e407f617c5f6603ddc0e5f295e730a310.png" src="../_images/b218467ab7f2993b432f2247fe4d5c4e407f617c5f6603ddc0e5f295e730a310.png" />
</div>
</div>
<p>But really, <span class="math notranslate nohighlight">\(P\)</span> lets us apply <em>any linear transformation</em> to the gradient. For our quadratic case, we actually know the ‘ideal’ transformation to use. Remember that each gradient update has a natural decomposition onto the eigenvector bases of <span class="math notranslate nohighlight">\(Q\)</span>. So, we can find a <span class="math notranslate nohighlight">\(P\)</span> which rotates the gradient onto the eigenvalue basis of <span class="math notranslate nohighlight">\(Q\)</span>, then apply the maximum learning rate for each eigen component.</p>
<p>Recall that error for each eigen component decreases with a multplier of <span class="math notranslate nohighlight">\(|1 - \alpha \lambda|\)</span>, so for the <strong>maximum per-coordinate learning rate</strong> we want this multiplier to be close to zero, i.e. <span class="math notranslate nohighlight">\(\alpha = 1 / \lambda\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">preconditioned_gradient_descent</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">grad_z</span><span class="p">):</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">eigenvectors</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">@</span> <span class="n">eigenvectors</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">P</span> <span class="o">@</span> <span class="n">grad_z</span>
<span class="n">plot_optim</span><span class="p">(</span><span class="n">preconditioned_gradient_descent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c22fef1a23b766643dd3ba19e98ae874b1e96267531b1bd3e240d2b75a7402ca.png" src="../_images/c22fef1a23b766643dd3ba19e98ae874b1e96267531b1bd3e240d2b75a7402ca.png" />
</div>
</div>
<p>With the analytically ideal preconditioning matrix, we can point <em>directly</em> to the solution, and arrive there in a single gradient step. One-step solution is a special property of quadratic loss functions, and should not be expected to generalize to more complicated losses. That said, preconditioning is a powerful tool that gives us a more accuate gradient direction. It turns out that preconditioning relates directly to <strong>second-order descent methods</strong>, which we’ll cover in a future section [].</p>
</section>
<section id="descent-on-neural-networks">
<h2>Descent on Neural Networks<a class="headerlink" href="#descent-on-neural-networks" title="Link to this heading">#</a></h2>
<p>To apply gradient descent to real neural networks, we’ll basically follow the same procedures as above, with two key differences.</p>
<p>The first difference is we won’t have a closed-form equation for our loss function. In machine learning we’ll typically have a large dataset of <code class="docutils literal notranslate"><span class="pre">x,</span> <span class="pre">y</span></code> pairs, and define our loss function to minimize some prediction error over <code class="docutils literal notranslate"><span class="pre">y</span></code>. This dataset is often so big that it’s unreasonable to try and calculate predictions over the entire dataset at once. Instead, we can sample random pairs from the dataset, and optimize the approximate loss we get from these pairs. This algorithm, known as <strong>stochastic gradient descent (SGD)</strong>, gives us an estimate of the true gradient at each step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">grads</span> <span class="o">*</span> <span class="n">lr</span> <span class="c1"># Stochastic Gradient Descent</span>
</pre></div>
</div>
<p>The second difference is that for multi-layer systems, training dynamics for each layer change as the <em>other layers are updated</em>. In the quadratic example above we knew how to find an optimal learning rate, and even an optimal preconditioning matrix. This won’t be true for neural network training, since each layer depends on the other layers, it’s even more critical to use an iterative procedure.</p>
<p>Let’s try out our simple SGD algorithm on a standard task, <strong>MNIST classification</strong>. The MNIST dataset is a set of handwritten numerical digits, represented as 28x28 matrices. We’ll collapse each matrix into a 784-length vector, then train a three-layer neural network to classify each digit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">),</span> <span class="p">(</span><span class="n">valid_images</span><span class="p">,</span> <span class="n">valid_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Dataset size:&#39;</span><span class="p">,</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">title</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Label: </span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset size: (60000, 28, 28)
</pre></div>
</div>
<img alt="../_images/a602be728ec9310f2e299eef2e74b80e5bf64b21fc6d960420d7b6a0ec68e980.png" src="../_images/a602be728ec9310f2e299eef2e74b80e5bf64b21fc6d960420d7b6a0ec68e980.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">sample_batch</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">batchsize</span><span class="p">,),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="p">(</span><span class="n">batchsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">learn_mnist</span><span class="p">(</span><span class="n">update_fn</span><span class="p">,</span> <span class="n">init_opt_state</span><span class="p">):</span>
    <span class="n">train_losses</span><span class="p">,</span> <span class="n">valid_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">param_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sample_batch</span><span class="p">(</span><span class="n">param_key</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
    <span class="n">v_images</span><span class="p">,</span> <span class="n">v_labels</span> <span class="o">=</span> <span class="n">sample_batch</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">valid_images</span><span class="p">,</span> <span class="n">valid_labels</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">param_key</span><span class="p">,</span> <span class="n">images</span><span class="p">)[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>
    <span class="n">opt_state</span> <span class="o">=</span> <span class="n">init_opt_state</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">apply</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">p</span><span class="p">},</span> <span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">10</span><span class="p">)[</span><span class="n">y</span><span class="p">]))</span>
    <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">data_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sample_batch</span><span class="p">(</span><span class="n">data_key</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span> <span class="o">=</span> <span class="n">update_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
        <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">valid_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">v_images</span><span class="p">,</span> <span class="n">v_labels</span><span class="p">))</span>
        <span class="c1"># if i % 100 == 0:</span>
        <span class="c1">#     print(f&#39;Iteration {i}, Train Loss: {train_losses[-1]}, Valid Loss: {valid_losses[-1]}&#39;)</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">axs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train Loss&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Valid Loss&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loss is below 0.15 at iteration&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.15</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">),</span> <span class="kc">None</span>
<span class="n">init_opt_state</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="kc">None</span>
<span class="n">learn_mnist</span><span class="p">(</span><span class="n">sgd</span><span class="p">,</span> <span class="n">init_opt_state</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e6a9b1d7747365f7e834a927f05ebd29ca640bdd019fc4a5524e75ffbd901c31.png" src="../_images/e6a9b1d7747365f7e834a927f05ebd29ca640bdd019fc4a5524e75ffbd901c31.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss is below 0.15 at iteration 825
</pre></div>
</div>
</div>
</div>
</section>
<section id="adaptive-learning-rate-adagrad">
<h2>Adaptive Learning Rate (Adagrad)<a class="headerlink" href="#adaptive-learning-rate-adagrad" title="Link to this heading">#</a></h2>
<p>In the quadratic example, we saw how we can get faster convergence by using different learning rates per coordinate. <strong>Adagrad</strong> gives us a method of doing the same for deep neural networks. Adagrad can be seen as a preconditioned gradient descent method, where <span class="math notranslate nohighlight">\(P\)</span> is a diagonal matrix. In other words, we will use an independent learning rate for <em>each parameter</em> in our network.</p>
<p>Recall the core issue – some parameters affect the loss more than others. We want to normalize for this. Adagrad achieves this by keeping track of <strong>per-parameter gradient variance</strong>. When taking each gradient step, we will divide the gradient elementwise by these historical variances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">adagrad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">variances</span><span class="p">):</span>
    <span class="n">scaled_grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">g</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">g</span> <span class="o">/</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="mf">1e-3</span><span class="p">)),</span> <span class="n">grads</span><span class="p">,</span> <span class="n">variances</span><span class="p">)</span>
    <span class="n">new_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">scaled_grads</span><span class="p">)</span>
    <span class="n">new_variances</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">v</span> <span class="o">+</span> <span class="n">g</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">new_variances</span>
<span class="n">init_variances</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">)</span>
<span class="n">learn_mnist</span><span class="p">(</span><span class="n">adagrad</span><span class="p">,</span> <span class="n">init_variances</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2002ada4f2604e457b38f75eb70207865342d5dcb343b24c269278d45cb57f9a.png" src="../_images/2002ada4f2604e457b38f75eb70207865342d5dcb343b24c269278d45cb57f9a.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss is below 0.15 at iteration 171
</pre></div>
</div>
</div>
</div>
</section>
<section id="momentum">
<h2>Momentum<a class="headerlink" href="#momentum" title="Link to this heading">#</a></h2>
<p>Variance in our gradient direction can come from two main sources. First, the nature of stochastic gradient descent means we sample a new batch every update, which can be seen as adding noise to our gradient. Second, updates that are too large can result in oscillations in gradient direction.</p>
<p>To reduce these variances, we can condition our update not just on the current batch, but also an average of <em>past</em> gradients, which we call <strong>momentum</strong>. This technique is also known as the <em>heavy ball method</em>, as it resembles our parameters having mass that maintains past directions.</p>
<p>A simple way to implement momentum is to keep track of an exponential moving average of the gradient by introducing a new momentum state <span class="math notranslate nohighlight">\(m\)</span>:</p>
<div class="math notranslate nohighlight">
\[
m' \leftarrow \beta m + \nabla L(\theta)
\theta' \leftarrow x' - \alpha m'
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">momentum_update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">momentum</span><span class="p">):</span>
    <span class="n">new_momentum</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">g</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
    <span class="n">new_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">new_momentum</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">new_momentum</span>
<span class="n">init_momentum</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">m</span><span class="p">)</span>
<span class="n">learn_mnist</span><span class="p">(</span><span class="n">momentum_update</span><span class="p">,</span> <span class="n">init_momentum</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ccaaf1029a65eb997e7dbd3da1e695f0dbf002f2ee345015054b3d7d8f419176.png" src="../_images/ccaaf1029a65eb997e7dbd3da1e695f0dbf002f2ee345015054b3d7d8f419176.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss is below 0.15 at iteration 83
</pre></div>
</div>
</div>
</div>
</section>
<section id="adam">
<h2>Adam<a class="headerlink" href="#adam" title="Link to this heading">#</a></h2>
<p>The most frequently used optimizer today is <strong>Adam</strong>, which combines insights from adaptive learning rates and momentum. Adam uses an exponential moving average to keep track of both past gradients (i.e. momentum) and past gradient <em>variances</em> (as we did in Adagrad):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
m' &amp; \leftarrow \beta_1 m + (1-\beta_1) \nabla L(\theta) \\
s' &amp; \leftarrow \beta_2 s + (1-\beta_2) (\nabla L(\theta))^2
\end{align}
\end{split}\]</div>
<p>Notice that the equations are slightly different then in Adagrad or momentum. The point of these changes is to keep the scales of the momentum and variance estimates consistent. Adagrad has a problem where keeping track of the sum of historical variances will rapidly go to infinity. Our simple momentum implementation is also weird, as the momentum magnitude will converge to approximately <span class="math notranslate nohighlight">\(1/(1-\beta)\)</span>. Instead, we would like to keep magnitudes normalized.</p>
<p>Because we initialize <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(s\)</span> to zero, at the beginning of training our estimates are closer to zero than we would like. Thus, in Adam we account for this and scale <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(s\)</span> by a scalar depending on training step:</p>
<div class="math notranslate nohighlight">
\[
\hat{m_t} = m_t / (1-\beta^t_1) \quad \text{and} \quad \hat{s_t} = s_t / (1-\beta^t_2)
\theta \leftarrow \theta - \alpha m / (\sqrt{s} + \eps)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta^t\)</span> represents <span class="math notranslate nohighlight">\(\beta\)</span> to the power <span class="math notranslate nohighlight">\(t\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">adam_update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">):</span>
    <span class="n">momentum</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">opt_state</span>
    <span class="n">new_momentum</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">g</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
    <span class="n">new_variance</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="n">g</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
    <span class="n">update</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">),</span> <span class="n">new_momentum</span><span class="p">,</span> <span class="n">new_variance</span><span class="p">)</span>
    <span class="n">new_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">u</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">u</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">update</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_params</span><span class="p">,</span> <span class="p">(</span><span class="n">new_momentum</span><span class="p">,</span> <span class="n">new_variance</span><span class="p">)</span>
<span class="n">init_opt_state</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">m</span><span class="p">),</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">m</span><span class="p">))</span>
<span class="n">learn_mnist</span><span class="p">(</span><span class="n">adam_update</span><span class="p">,</span> <span class="n">init_opt_state</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9407473c9257234a669bc9721171c965861c11d7b623c7f3d661cc08eef7ef77.png" src="../_images/9407473c9257234a669bc9721171c965861c11d7b623c7f3d661cc08eef7ef77.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss is below 0.15 at iteration 29
</pre></div>
</div>
</div>
</div>
</section>
<section id="summary-of-optimization-results">
<h2>Summary of optimization results<a class="headerlink" href="#summary-of-optimization-results" title="Link to this heading">#</a></h2>
<p>Iteration where loss goes below 0.15:</p>
<ul class="simple">
<li><p>SGD: 825</p></li>
<li><p>Adagrad: 171</p></li>
<li><p>Momentum: 83</p></li>
<li><p><strong>Adam: 29</strong></p></li>
</ul>
<p>So, there’s a reason why Adam is the most popular algorithm today. It’s reliable, converges fast, and does not require much hyperparamter tuning. The main downside to Adam is it requires storing 3x the parameter count (parameters, momentum, variance) which can consume more memory.</p>
<p>For more on optimization, check out the page on [second order optimization methods] (todo).</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./2-training-neural-networks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="backpropagation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Backpropagation</p>
      </div>
    </a>
    <a class="right-next"
       href="../7-notes/model-agnostic-meta-learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model Agnostic Meta Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-quadratic-loss-function">Example: Quadratic loss function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-in-terms-of-eigenvalues">Learning rate in terms of eigenvalues</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preconditioning">Preconditioning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#descent-on-neural-networks">Descent on Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-learning-rate-adagrad">Adaptive Learning Rate (Adagrad)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum">Momentum</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adam">Adam</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-optimization-results">Summary of optimization results</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kevin Frans
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>