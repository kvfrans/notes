
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Model Agnostic Meta Learning &#8212; An Alchemist&#39;s Guide to Deep Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=9a378b06" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '7-notes/model-agnostic-meta-learning';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Optimization" href="../2-training-neural-networks/optimization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">An Alchemist's Guide to Deep Learning</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Toolkit</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1-numerical-toolkit/overview-of-jax.html">Overview of JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-numerical-toolkit/training-with-flax.html">Building NNs with Flax</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Training Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../2-training-neural-networks/neural-networks.html">Why Neural Networks?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-training-neural-networks/backpropagation.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-training-neural-networks/optimization.html">Optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Model Agnostic Meta Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F7-notes/model-agnostic-meta-learning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/7-notes/model-agnostic-meta-learning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model Agnostic Meta Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sine-wave-regression">Example: Sine wave regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#under-the-hood">Under the hood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-order-maml-fomaml">First-Order MAML (FOMAML)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reptile">Reptile</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="model-agnostic-meta-learning">
<h1>Model Agnostic Meta Learning<a class="headerlink" href="#model-agnostic-meta-learning" title="Link to this heading">#</a></h1>
<p><strong>Meta-learning</strong> is the idea of learning <em>how to learn</em>. Meta-learning is often assossiated with the <strong>few-shot learning</strong> challenge – how can a network learn to quickly solve new tasks, when given only a few <span class="math notranslate nohighlight">\((&lt; 100)\)</span> examples? Whereas in standard machine learning we have a dataset of data points, in few-shot learning we generally want a dataset of <em>tasks</em>. For example, the character classification <em>meta</em>-task is to classify a new set of K characters, given a few examples of each class. We’ll often call this final evaluation the <strong>downstream</strong> task.</p>
<p>In the specific example of <strong>model agnostic meta learning (MAML)</strong>, we’d like to learn a set of neural network parameters such that a small number of gradient descent steps is enough to solve a new task. The intution here is that gradient descent is good, and if we have some data for our downstream task, we should run gradient descent on it. Some kinds of parameters are inherently good at <em>adapting</em> to new tasks, and we want to find these parameters.</p>
<p>Imagine a typical <em>inner-loop</em> optimization for the character classification task. We’re given a set of task-specific input-output pairs (e.g. images and class labels), and we will run gradient descent. To evaluate, we will test our new model on some held-out validation data from the same task.</p>
<p>MAML tries to find a set of global parameters which are a good starting point for the above inner-loop process. It achieves this by taking a gradient <em>through</em> the optimization process.</p>
<section id="example-sine-wave-regression">
<h2>Example: Sine wave regression<a class="headerlink" href="#example-sine-wave-regression" title="Link to this heading">#</a></h2>
<p>We’ll take an example from the MAML paper. Given a few points from a sine wave of random phase and amplitude, the task is to predict the rest of the points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># From: https://github.com/gcucurull/maml_flax</span>
<span class="k">def</span> <span class="nf">generate_sinusoids</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">num_tasks</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
    <span class="n">amp_key</span><span class="p">,</span> <span class="n">phase_key</span><span class="p">,</span> <span class="n">x_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">amp</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">amp_key</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">phase</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">phase_key</span><span class="p">,</span> <span class="n">minval</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">samples_x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">x_key</span><span class="p">,</span> <span class="n">minval</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">samples_y</span> <span class="o">=</span> <span class="n">amp</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">samples_x</span> <span class="o">+</span> <span class="n">phase</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samples_x</span><span class="p">,</span> <span class="n">samples_y</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_sinusoids</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">axs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Task </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x2b0678ac0&gt;
</pre></div>
</div>
<img alt="../_images/7c605e73f52885abda439464fd57b6155a53e1b7580f02329d3d73e9e40d8d49.png" src="../_images/7c605e73f52885abda439464fd57b6155a53e1b7580f02329d3d73e9e40d8d49.png" />
</div>
</div>
<p>Let’s start by defining a simple two-layer neural network, and overfitting to one of the downstream tasks. We’ll sample a single sine wave, generate some data from it, then try to fit the function.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">40</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">40</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">TrainState</span><span class="p">(</span><span class="n">flax</span><span class="o">.</span><span class="n">struct</span><span class="o">.</span><span class="n">PyTreeNode</span><span class="p">):</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">flax</span><span class="o">.</span><span class="n">struct</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">pytree_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">opt</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">flax</span><span class="o">.</span><span class="n">struct</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">pytree_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Any</span>
    <span class="n">opt_state</span><span class="p">:</span> <span class="n">Any</span>

<span class="k">def</span> <span class="nf">init_train_state</span><span class="p">():</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">TrainState</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">opt</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">opt_state</span><span class="o">=</span><span class="n">opt</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">params</span><span class="p">),</span> <span class="n">module</span><span class="o">=</span><span class="n">network</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">update_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">grad_params</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">grad_params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>

    <span class="n">l</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">updates</span><span class="p">,</span> <span class="n">opt_state</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt_state</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
    <span class="n">train_state</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="o">=</span><span class="n">opt_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_state</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">l</span><span class="p">}</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train network on a *single* sine wave.</span>
<span class="n">train_state</span> <span class="o">=</span> <span class="n">init_train_state</span><span class="p">()</span> <span class="c1"># Make a new model.</span>
<span class="n">x_all</span><span class="p">,</span> <span class="n">y_all</span> <span class="o">=</span> <span class="n">generate_sinusoids</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">id_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">batch_ids</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">id_key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,),</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">x_all</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x_all</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">],</span> <span class="n">y_all</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">]</span>
    <span class="n">train_state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">update_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Evaluate.</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">x_all</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_all</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_all</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_all</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prediction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d975777d1a058dbd5fb757385b12aa198ecefdd8b397541be4c99d93311acd05.png" src="../_images/d975777d1a058dbd5fb757385b12aa198ecefdd8b397541be4c99d93311acd05.png" />
</div>
</div>
<p>Cool, so we can overfit to a single sine wave pretty easily.</p>
<p>Next, let’s see how our network works on the few-shot task. We’ll use a naive baseline – just running SGD on samples taken from <em>random</em> tasks, i.e. different sine waves. We’ll train our network this way, then to evaluate, we will take a few steps of gradient descent on a <em>new</em> sine wave, and see how good our network adapts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_single</span><span class="p">,</span> <span class="n">y_single</span> <span class="o">=</span> <span class="n">generate_sinusoids</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>

<span class="c1"># Train network on a *all* sine waves.</span>
<span class="n">train_state</span> <span class="o">=</span> <span class="n">init_train_state</span><span class="p">()</span> <span class="c1"># Make a new model.</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">id_key</span><span class="p">,</span> <span class="n">sine_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_sinusoids</span><span class="p">(</span><span class="n">sine_key</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">train_state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">update_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">x_single</span><span class="p">)</span>

<span class="c1"># 10-step finetune on a single sine wave.</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">train_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">update_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">32</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">32</span><span class="p">])</span>
<span class="n">y_pred_finetune</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">x_single</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred_all</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Base Prediction&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred_finetune</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Finetuned Prediction&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Naive Baseline&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/03197a176d99a98fcafa621b869abc9c4075436ba288eca0d708105edddfeb5c.png" src="../_images/03197a176d99a98fcafa621b869abc9c4075436ba288eca0d708105edddfeb5c.png" />
</div>
</div>
<p>The base prediction here is just all zeros. This should make sense; the average of all sine waves with phase <span class="math notranslate nohighlight">\((-\pi, pi)\)</span> is just the zero line after all. When we do some finetuning on our downstream task, the results are not great. We’re unable to quickly adapt to the new sine wave.</p>
<p>Why does this happen? Somemtimes, learning the average over all tasks is a significantly different problem than learning each task individually. This behavior is especially true in our example – the average is just a straight line! The features of our neural network don’t need to learn anything smart about sine wave structure.</p>
<p>Next, let’s try to tackle the problem using MAML. A basic outline of MAML is as follows:</p>
<ul class="simple">
<li><p>We start by sampling a set of <span class="math notranslate nohighlight">\(K\)</span> tasks. In our case, this is different sine waves. For each task, we’ll generate a set of training data and validation data.</p></li>
<li><p>In the <em>inner loop</em>, we take our global parameters and take a gradient step on our task-specific training data. This give us updated <em>task-specific</em> parameters. Then, we evaluate loss of the task-specific parameters on the validation data. This is done in parallel for all <span class="math notranslate nohighlight">\(K\)</span> tasks.</p></li>
<li><p>In the <em>outer loop</em>, we take the gradient with respect to global parameters of the inner-loop validation loss. This means taking a gradient <em>of the inner gradient step</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">update_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span> <span class="c1"># MAML Update function.</span>
    <span class="n">inner_steps</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">def</span> <span class="nf">maml_loss</span><span class="p">(</span><span class="n">task_params</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">val_y</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">task_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inner_steps</span><span class="p">):</span> <span class="c1"># Inner loop.</span>
            <span class="n">model_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">task_loss</span><span class="p">)(</span><span class="n">task_params</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
            <span class="n">task_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">task_params</span><span class="p">,</span> <span class="n">model_grad</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">task_loss</span><span class="p">(</span><span class="n">task_params</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">val_y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">grad_params</span><span class="p">):</span> <span class="c1"># Parallel map over K inner-loop tasks.</span>
        <span class="n">maml_losses</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">task</span><span class="p">:</span> <span class="n">maml_loss</span><span class="p">(</span><span class="n">grad_params</span><span class="p">,</span> <span class="o">*</span><span class="n">task</span><span class="p">))(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">maml_losses</span><span class="p">)</span>

    <span class="n">l</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span> <span class="c1"># Outer-loop gradient.</span>
    <span class="n">updates</span><span class="p">,</span> <span class="n">opt_state</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt_state</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
    <span class="n">train_state</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="o">=</span><span class="n">opt_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_state</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">l</span><span class="p">}</span>

<span class="c1"># Train network on a *all* sine waves.</span>
<span class="n">train_state</span> <span class="o">=</span> <span class="n">init_train_state</span><span class="p">()</span> <span class="c1"># Make a new model.</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">id_key</span><span class="p">,</span> <span class="n">sine_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_sinusoids</span><span class="p">(</span><span class="n">sine_key</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1"># 16 tasks, with 64 datapoints each.</span>
    <span class="n">inner_train_x</span><span class="p">,</span> <span class="n">inner_val_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">inner_train_y</span><span class="p">,</span> <span class="n">inner_val_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">train_state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">update_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="p">(</span><span class="n">inner_train_x</span><span class="p">,</span> <span class="n">inner_train_y</span><span class="p">,</span> <span class="n">inner_val_x</span><span class="p">,</span> <span class="n">inner_val_y</span><span class="p">))</span>
<span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">x_single</span><span class="p">)</span>

<span class="c1"># 10-step finetune on a single sine wave.</span>
<span class="k">def</span> <span class="nf">few_step_finetune</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">l</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_state</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">train_state</span> <span class="o">=</span> <span class="n">few_step_finetune</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">64</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">64</span><span class="p">])</span>
<span class="n">y_pred_finetune</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">x_single</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred_all</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Base Prediction&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred_finetune</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Finetuned Prediction&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;MAML Prediction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fff9b2aa6778fa9b28eab4e31d30d54dd56c89190773bb47d82c4136bfc27398.png" src="../_images/fff9b2aa6778fa9b28eab4e31d30d54dd56c89190773bb47d82c4136bfc27398.png" />
</div>
</div>
<p>So, that works a lot better. Our base predictions look the same as the naive basline, but after a few finetuning steps, the MAML parameters do a much better job on the downstream task.</p>
</section>
<section id="under-the-hood">
<h2>Under the hood<a class="headerlink" href="#under-the-hood" title="Link to this heading">#</a></h2>
<p>What is happening mathematically behind the scenes? Let’s examine a MAML update for a single inner task.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    grad(\theta) &amp; = \nabla_\theta \; L_\text{valid}(\theta + \alpha \nabla_\theta \; L_\text{train}(\theta)) \\
    &amp; = \nabla_\theta \; L_\text{valid}(U(\theta)) \qquad \qquad \qquad \qquad \text{where} \; U(\theta) = \theta - \alpha \nabla_\theta \; L_\text{train}(\theta) \\
    &amp; = \nabla_{U(\theta)} \; L_\text{valid}(U(\theta)) \; \cdot \;  \nabla_\theta \; U(\theta) \\
    &amp; = \nabla_{U(\theta)} \; L_\text{valid}(U(\theta)) \; \cdot \;  \nabla_\theta \; (\theta - \alpha \nabla_\theta \; L_\text{train}(\theta)) \\
    &amp; = \nabla_{U(\theta)} \; L_\text{valid}(U(\theta)) \; \cdot \;  (I - \alpha \nabla^2_\theta \; L_\text{train}(\theta)) \\
\end{align}
\end{split}\]</div>
<p>So, we’re computing two main terms. The first is the gradient with respect to validation loss, on the <em>updated</em> inner loop parameters. This is relatively cheap to compute and is a vector <span class="math notranslate nohighlight">\(R^n\)</span>. The second term involves a second-order gradient (Hessian matrix) of size <span class="math notranslate nohighlight">\(R^{n,n}\)</span>. This second term is expensive; it grows quadratically with respect to parameter count.</p>
</section>
<section id="first-order-maml-fomaml">
<h2>First-Order MAML (FOMAML)<a class="headerlink" href="#first-order-maml-fomaml" title="Link to this heading">#</a></h2>
<p>Calculating the Hessian over our entire neural network is expensive. For the small networks we worked with above, it’s fine, for but real neural network this is impractical. <strong>First-order MAML</strong> makes an approximation to the MAML update – what if we just discard this term? We can approximate <span class="math notranslate nohighlight">\(\nabla^2_\theta \; L_\text{train}(\theta) = 0\)</span>, which drops the second term entirely.</p>
<p>We can implement this change by using <code class="docutils literal notranslate"><span class="pre">stop_gradient</span></code> after we calculate our inner-loop gradient update.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inner_steps</span><span class="p">):</span>
    <span class="n">model_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">task_loss</span><span class="p">)(</span><span class="n">task_params</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
    <span class="n">model_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">model_grad</span><span class="p">)</span> <span class="c1"># (FOMAML: Stop gradient here.)</span>
    <span class="n">task_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">task_params</span><span class="p">,</span> <span class="n">model_grad</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">update_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span> <span class="c1"># MAML Update function.</span>
    <span class="n">inner_steps</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">def</span> <span class="nf">maml_loss</span><span class="p">(</span><span class="n">task_params</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">val_y</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">task_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inner_steps</span><span class="p">):</span> <span class="c1"># Inner loop.</span>
            <span class="n">model_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">task_loss</span><span class="p">)(</span><span class="n">task_params</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
            <span class="n">model_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">model_grad</span><span class="p">)</span> <span class="c1"># (ONLY CHANGE: Stop gradient here.)</span>
            <span class="n">task_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">task_params</span><span class="p">,</span> <span class="n">model_grad</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">task_loss</span><span class="p">(</span><span class="n">task_params</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">val_y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">grad_params</span><span class="p">):</span> <span class="c1"># Parallel map over K inner-loop tasks.</span>
        <span class="n">maml_losses</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">task</span><span class="p">:</span> <span class="n">maml_loss</span><span class="p">(</span><span class="n">grad_params</span><span class="p">,</span> <span class="o">*</span><span class="n">task</span><span class="p">))(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">maml_losses</span><span class="p">)</span>

    <span class="n">l</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span> <span class="c1"># Outer-loop gradient.</span>
    <span class="n">updates</span><span class="p">,</span> <span class="n">opt_state</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt_state</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
    <span class="n">train_state</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="o">=</span><span class="n">opt_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_state</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">l</span><span class="p">}</span>

<span class="c1"># Train network on a *all* sine waves.</span>
<span class="n">train_state</span> <span class="o">=</span> <span class="n">init_train_state</span><span class="p">()</span> <span class="c1"># Make a new model.</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">id_key</span><span class="p">,</span> <span class="n">sine_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_sinusoids</span><span class="p">(</span><span class="n">sine_key</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1"># 16 tasks, with 64 datapoints each.</span>
    <span class="n">inner_train_x</span><span class="p">,</span> <span class="n">inner_val_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">inner_train_y</span><span class="p">,</span> <span class="n">inner_val_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">train_state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">update_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="p">(</span><span class="n">inner_train_x</span><span class="p">,</span> <span class="n">inner_train_y</span><span class="p">,</span> <span class="n">inner_val_x</span><span class="p">,</span> <span class="n">inner_val_y</span><span class="p">))</span>
<span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">x_single</span><span class="p">)</span>

<span class="c1"># 10-step finetune on a single sine wave.</span>
<span class="k">def</span> <span class="nf">few_step_finetune</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">l</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_state</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">train_state</span> <span class="o">=</span> <span class="n">few_step_finetune</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">64</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">64</span><span class="p">])</span>
<span class="n">y_pred_finetune</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">x_single</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred_all</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Base Prediction&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred_finetune</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Finetuned Prediction&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;First-Order MAML Prediction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/0397d0d4540ec50a92740c7bcfd8288011bb707969ee113220dd1f344bfed35d.png" src="../_images/0397d0d4540ec50a92740c7bcfd8288011bb707969ee113220dd1f344bfed35d.png" />
</div>
</div>
</section>
<section id="reptile">
<h2>Reptile<a class="headerlink" href="#reptile" title="Link to this heading">#</a></h2>
<p>Given that first-order MAML works decently well, <strong>Reptile</strong> is an extension that extends FOMAML to support multi-step inner loops. The idea is as follows:</p>
<ul class="simple">
<li><p>In the <em>inner loop</em>, take N gradient steps on task-specific data to get new task-specific parameters.</p></li>
<li><p>In the <em>outer loop</em>, interpolate between the global parameters and the task-specific parameters.</p></li>
</ul>
<p><strong>If N=1</strong>, then Reptile is just equivalent to regular gradient descent (i.e. our naive implementation earlier).</p>
<p><strong>If N=2</strong>, Reptile looks more like FOMAML, with the slight difference that both inner-loop gradients are propagated back to the global parameters (as opposed to only the last validation gradient in FOMAML). We also don’t have a separate training/validation split for each task. We just view all the data as the same.</p>
<p><strong>As N &gt; 2</strong>, Reptile can be seen as a way to empirically estimate second-order behavior through repeated iteration. Instead of computing the explicit Hessian, we simply take many steps during the inner loop, then propogate the <em>sum</em> of those changes back to the global parameters. Intuitively, this leads the global parameters to a point that is close to the optimal parameters for any downstream task.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">update_fn_reptile</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">inner_steps</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">def</span> <span class="nf">inner_reptile_params</span><span class="p">(</span><span class="n">task_params</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">task_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inner_steps</span><span class="p">):</span>
            <span class="n">inner_batchsize</span> <span class="o">=</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">inner_steps</span>
            <span class="n">inner_x</span> <span class="o">=</span> <span class="n">train_x</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">inner_batchsize</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">inner_batchsize</span><span class="p">]</span>
            <span class="n">inner_y</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">inner_batchsize</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">inner_batchsize</span><span class="p">]</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">model_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">task_loss</span><span class="p">)(</span><span class="n">task_params</span><span class="p">,</span> <span class="n">inner_x</span><span class="p">,</span> <span class="n">inner_y</span><span class="p">)</span>
            <span class="n">task_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">task_params</span><span class="p">,</span> <span class="n">model_grad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">task_params</span>

    <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span> <span class="o">=</span> <span class="n">batch</span> <span class="c1"># (32, 10, 1).</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">rparams</span> <span class="o">=</span> <span class="n">inner_reptile_params</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">train_x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">train_y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">stepsize</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1"># In paper, this number is annealed from one to zero.</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">stepsize</span><span class="p">)</span> <span class="o">+</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="n">stepsize</span><span class="p">),</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">rparams</span><span class="p">)</span> 
    <span class="n">train_state</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_state</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train network on a *all* sine waves.</span>
<span class="n">train_state</span> <span class="o">=</span> <span class="n">init_train_state</span><span class="p">()</span> <span class="c1"># Make a new model.</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">id_key</span><span class="p">,</span> <span class="n">sine_key</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_sinusoids</span><span class="p">(</span><span class="n">sine_key</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span> <span class="c1"># 1 tasks, with 256 datapoints.</span>
    <span class="n">train_state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">update_fn_reptile</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="p">(</span><span class="n">inner_train_x</span><span class="p">,</span> <span class="n">inner_train_y</span><span class="p">))</span>
<span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">x_single</span><span class="p">)</span>

<span class="c1"># 10-step finetune on a single sine wave.</span>
<span class="k">def</span> <span class="nf">few_step_finetune</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">l</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_state</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">train_state</span> <span class="o">=</span> <span class="n">few_step_finetune</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">64</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">64</span><span class="p">])</span>
<span class="n">y_pred_finetune</span> <span class="o">=</span> <span class="n">train_state</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">x_single</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred_all</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Base Prediction&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground truth&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_single</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred_finetune</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Finetuned Prediction&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Reptile Prediction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/4186f1e082c5e4d74fd88ea4f2c3ab57f713e74f17563ecde56a1cddc7bb7f74.png" src="../_images/4186f1e082c5e4d74fd88ea4f2c3ab57f713e74f17563ecde56a1cddc7bb7f74.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./7-notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../2-training-neural-networks/optimization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Optimization</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sine-wave-regression">Example: Sine wave regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#under-the-hood">Under the hood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-order-maml-fomaml">First-Order MAML (FOMAML)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reptile">Reptile</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kevin Frans
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>