{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Alchemist's Notes on Deep Learning\n",
    "\n",
    "I have recently had the opportunity to spend lots of time learning with the excuse of [pursing a PhD](https://bair.berkeley.edu). These **Alchemist's Notes** are a byproduct of that process. Each page contains notes and ideas related broadly to deep learning, generative modelling, and practical engineering. I've actually been writing these things [since 2016 on my personal website](https://kvfrans.com/), but this site should be a more put-together version.\n",
    "\n",
    "**Who is this site for?** For you, I hope. The ideal reader has at least an undergraduate-level understanding of machine learning, and is comfortable with Python. The rest you can figure out as you go.\n",
    "\n",
    "**What do the contents look like?** The main goal of these notes is to provide definitions and examples. I have found these to be the most critical bits to convey when introducing new concepts. Each page will give a brief overview and example implementation, then we will mostly be answering commonly-posed questions. Wherever possible, we will utilize concrete code examples. This website is compiled from a set of Jupyter notebooks, so **you can go and play through the code on every page**. (Click the 'Colab' button on the top-right). \n",
    "\n",
    "**What's with 'Alchemy' in the name?** Because in deep learning, we have not arrived at a grand theory of everything. We do have snippets of evidence and intutions, and examples of things that work. We have insights from mathematical foundations and theories. We have a rich body of literature and open-source code. Yet, it is an open question how all these ideas should come together. Deep learning is still in the alchemical age, and what follows should be seen as a reference guide and not the final solutions. I ask you to come to your own conclusions!\n",
    "\n",
    "These notes are not finished. I am planning to continuously update it. No guarantees that the content will stay on track. If you see any issues or have suggestions, send me a message: kvfrans@berkeley.edu.\n",
    "\n",
    "The **Misc** section contains pages that are rougher and unrefined. You should view them more as experimental notes rather than a thorough explanation. Don't blame me if things are messy there.\n",
    "\n",
    "## More Resources\n",
    "\n",
    "This guide takes inspiration and information from many sources. Listed here are some great additional resources for learning.\n",
    "- [Deep Learning Book](https://www.deeplearningbook.org/) (Goodfellow, Bengio, Courville)\n",
    "- [Dive into Deep Learning (d2l)](https://d2l.ai/) (Zhang, Lipton, Li, Smola)\n",
    "- [Berkeley Deep Unsupervised Learning Course](https://sites.google.com/view/berkeley-cs294-158-sp24/home) (Abbeel, Yan, Frans, Wu)\n",
    "- [Probabalistic Machine Learning: Advanced Topics](https://probml.github.io/pml-book/book2.html) (Murphy)\n",
    "- [Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html) (Karpathy)\n",
    "- [Understanding Deep Learning](https://udlbook.github.io/udlbook/) (Prince)\n",
    "\n",
    "## Planned Table of Contents\n",
    "- About\n",
    "- Numerical Toolkit (JAX)\n",
    "    - Overview of JAX\n",
    "    - Training neural nets with Flax\n",
    "    - Logging\n",
    "    - Checkpointing\n",
    "- Training Neural Networks\n",
    "    - Neural Networks\n",
    "    - Backpropagation\n",
    "    - Optimizers\n",
    "- Building Blocks\n",
    "    - Linear Layers\n",
    "    - Activations\n",
    "    - Convolutional Layers\n",
    "    - Recurrent Networks\n",
    "    - Attention\n",
    "    - Residual Layers\n",
    "    - The Transformer\n",
    "- Models\n",
    "    - Classifiers (overfitting)\n",
    "    - Variational Auto-Encoders\n",
    "    - Generative Adversarial Networks\n",
    "    - Diffusion Models\n",
    "    - Autoregressive Models (LLMs)\n",
    "    - Representation Learning\n",
    "- Practical Scaling\n",
    "    - Structure of the GPU\n",
    "    - Dataloading\n",
    "    - Multi-GPU Parallelism\n",
    "    - Mixed Precision\n",
    "- Reinforcement Learning (Maybe?)\n",
    "    - Markov Decision Processes\n",
    "    - Policy Gradients\n",
    "    - Temporal Difference Learning\n",
    "    - Offline RL\n",
    "    - Goal-Conditioned RL\n",
    "    - Model-Based RL\n",
    "    - RL for LLMs\n",
    "- Theories\n",
    "    - Probablistic Models\n",
    "    - Matrices and Tensors"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
